# Quantamind Intern Fine-Tuning LLMs
You can run from Google Colab here: [Quantamind_Intern_Fine_Tuning_LLMs.ipynb](https://colab.research.google.com/drive/1viDzLJ1wQOq7FWUoa-D6kwl2jheB4vYV?usp=sharing).

## Project description
Fine-Tuning Llama 3.2 for text generation using two datasets and different methods.
Fine-Tuning Llama 3.2 model for text generation using QLoRa method and Supervised Fine-tuning Trainer (SFT) method with [Maxime Labonne's FineTome-100k dataset](https://huggingface.co/datasets/mlabonne/FineTome-100k) in ShareGPT style and inference method, other approach using QLoRa method and GRPO Trainer method with [OpenAI's famous GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k) and inference method.


## Model & dataset used

Model used: Llama-3.2-1B-Instruct from [unsloth ai](https://unsloth.ai/) (``` model_name = "unsloth/Llama-3.2-1B-Instruct" ```).

[Maxime Labonne's FineTome-100k dataset](https://huggingface.co/datasets/mlabonne/FineTome-100k) in ShareGPT style used for QLoRa method and Supervised Fine-tuning Trainer (SFT) method, the FineTome dataset is a subset of [arcee-ai/The-Tome](https://huggingface.co/datasets/arcee-ai/The-Tome) (without arcee-ai/qwen2-72b-magpie-en), re-filtered using [HuggingFaceFW/fineweb-edu-classifier](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier).The Tome [arcee-ai/The-Tome](https://huggingface.co/datasets/arcee-ai/The-Tome) (without arcee-ai/qwen2-72b-magpie-en) is a curated dataset designed for training large language models with a focus on instruction following. It was used in the training of our Arcee-Nova/Spark models, which was later merged with Qwen2-72B-Instruct (or 7B with the Spark model).


[OpenAI's famous GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k): Dataset Card for GSM8K. Dataset Summary.
GSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.

- These problems take between 2 and 8 steps to solve.
- Solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the final answer.
- A bright middle school student should be able to solve every problem: from the paper, "Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable."
- Solutions are provided in natural language, as opposed to pure math expressions. From the paper: "We believe this is the most generally useful data format, and we expect it to shed light on the properties of large language models’ internal monologues""

Dataset Structure: 
```
{
    'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',
    'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n#### 72',
}
```


Citation Information:
``` @article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
 ```


## Explanation of both methods

QLoRa method : 4-bit fine-tuning (```load_in_4bit = True # Use 4bit quantization to reduce memory usage (QLoRa). Can be False. ```). Slightly slower and marginally less accurate, but uses much less VRAM (4× less). 

You can try with LoRA method: 16-bit fine-tuning. It's slightly faster and slightly more accurate, but consumes significantly more VRAM (4× more than QLoRA). Recommended for 16-bit environments and scenarios where maximum accuracy is required.

[Supervised fine-tuning (SFT)](https://huggingface.co/docs/trl/sft_trainer) method is the most common step in post-training foundation models, and also one of the most effective. In TRL, we provide a simple API to train models with SFT in a few lines of code, for example as 
```python
training_args = SFTConfig(
    max_length=...,
    output_dir=...,
    ...
)
trainer = SFTTrainer(
    model="...", # example of model/LLM: model="facebook/opt-350m"
    train_dataset=...,
    args=...,
    ...
)
trainer.train()
```

[GRPO](https://huggingface.co/docs/trl/grpo_trainer) method is an online learning algorithm, meaning it improves iteratively by using the data generated by the trained model itself during training. The intuition behind GRPO objective is to maximize the advantage of the generated completions, while ensuring that the model remains close to the reference policy. To understand how GRPO works, it can be broken down into four main steps: Generating completions, computing the advantage, estimating the KL divergence, and computing the loss.
Below is an example script to train the model:
```python
training_args = GRPOConfig(output_dir=...)
trainer = GRPOTrainer(
    model="...", # example of model/LLM:  model="Qwen/Qwen2-0.5B-Instruct"
    reward_funcs=...,
    args=...,
    train_dataset=...,
    ...
)
trainer.train()
```

Inference method: To test the model after fine-tuning, we need to activate fast inference. Inference is the phase where a trained AI model generates outputs that are completely original using the context of the prompt.


## Steps to run the code

You can run from Google Colab here: [Quantamind_Intern_Fine_Tuning_LLMs.ipynb](https://colab.research.google.com/drive/1viDzLJ1wQOq7FWUoa-D6kwl2jheB4vYV?usp=sharing).
You can access the Google Colab from this link [Quantamind_Intern_Fine_Tuning_LLMs.ipynb](https://colab.research.google.com/drive/1viDzLJ1wQOq7FWUoa-D6kwl2jheB4vYV?usp=sharing) and after make a copy to run in your Google Colab account.

You can run as a [python script](https://github.com/AslauAlexandru/Quantamind-Intern-Fine-Tuning-LLMs/blob/main/quantamind_intern_fine_tuning_llms.py): 
```python

python quantamind_intern_fine_tuning_llms.py
```

## Results and performance comparison


After Fine-Tuning unsloth/Llama-3.2-1B-Instruct model for text generation using QLoRa method and Supervised Fine-tuning Trainer (SFT) method with [Maxime Labonne's FineTome-100k dataset](https://huggingface.co/datasets/mlabonne/FineTome-100k), we obtain the best training loss: at step 16 we have the best training loss with 0.743100.
We have set the SFTTrainer trainer: Unsloth - 2x faster free finetuning | Num GPUs used = 1, Num examples = 100,000 | Num Epochs = 1 | Total steps = 60, Batch size per device = 2 | Gradient accumulation steps = 4, Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8 and Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained) due to QLoRa, 60/60 steps, 02:47 minutes, Epoch 0/1.

Inference output:

```

"role": "user", "content": "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,"

Output: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 198581, 325251, 526702, 843675, 1395076, 2175761, 3495507, 5773906, 9660250,

```


After Fine-Tuning unsloth/Llama-3.2-1B-Instruct model for text generation using QLoRa method and GRPO Trainer method with [OpenAI's famous GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), we obtain the best trainig loss, reward and reward_std: for all steps 20 we have the best training loss with 0.000000, at steps 1, 3, 4, 9, 12, 13, 16 and we have the reward with -2.500000 (the best reward), at step 2 and we have the reward with 0.216506 (the best reward_std). We have set the GRPOTrainer trainer: Unsloth - 2x faster free finetuning | Num GPUs used = 1, Num examples = 7,473 | Num Epochs = 1 | Total steps = 20, Batch size per device = 4 | Gradient accumulation steps = 4, Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16, Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained) due to QLoRa, 20/20 steps, 29:15 minutes, Epoch 0/1.

GRPOTrainer trainer, an example output:
```

******************** Question:
Janelle had 26 green marbles. Then she bought 6 bags of blue marbles. There were 10 marbles in each bag.  She created a gift of 6 green marbles and 8 blue marbles and gave it to a friend. How many marbles does Janelle have now? 
Answer:
72 
Response:
To find the total number of marbles Janelle had initially, we need to multiply the number of green marbles by the number of blue marbles and then add it to the initial number of green marbles.

Initial green marbles = 26
Initial blue marbles = 0 (since she didn't buy any blue marbles)
Total initial marbles = 26 + 0 = 26

She bought 6 bags of blue marbles, each containing 10 marbles. So, the total number of blue marbles is 6 * 10 = 60.

Now, we need to add the total number of blue marbles to the total initial marbles:
Total marbles = 26 + 60 = 86

After giving a gift of 6 green marbles and 8 blue marbles to her friend, Janelle now has:
Marbles left = 86 - 6 - 8 = 72

So, Janelle now has 72 marbles. 
Extracted:
,
******************** 

```


Inference output:

```

"role": "user", "content": "What is the sqrt of 101?"

Output: To find the square root of 101, we can use a calculator or an online math tool. 

Using a calculator: The square root of 101 is approximately 10.05.

Using an online math tool, the square root of 101 is approximately 10.097937.

Therefore, the square root of 101 is between 10.05 and 10.097937.<|eot_id|>

```


## Final conclusion and recommendation

In colusion Fine-Tuning unsloth/Llama-3.2-1B-Instruct model for text generation give us good results with respective methods, but for the best results or to improve the results, you can try to fine-tune with different LLMs, instead of using QLoRa use LoRa, increase total steps more than it is in present, use more than one GPU (optional) and increase or decrease batch size per device, gradient accumulation steps and other parameters, for more information please check this link [Fine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide/lora-hyperparameters-guide).








